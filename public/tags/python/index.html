<!DOCTYPE html>
<html><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="/manifest.json">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#b100b2">
    <meta name="theme-color" content="#b100b2">

    <meta property="og:url" content="https://martinjc.com/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Martin Chorley">
    <meta property="og:site_name" content="Martin Chorley">
    <meta property="og:locale" content="en_GB">
    <meta property="article:author" content="Martin Chorley">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:creator" content="@martinjc">
    <meta name="twitter:url" content="https://martinjc.com">
    <meta name="twitter:title" content="Martin Chorley">

    <link rel="icon" href="/img/favicon.png">
    <link href="https://fonts.googleapis.com/css?family=Roboto|Scope+One" rel="stylesheet">
    <link rel="stylesheet" href="/css/main.css">
</head><body><header>
    <div class="social">
        <ul>
            <li><a href="https://www.twitter.com/martinjc" class="icon twitter" title="Twitter"><svg viewBox="0 0 512 512">
                        <path d="M419.6 168.6c-11.7 5.2-24.2 8.7-37.4 10.2 13.4-8.1 23.8-20.8 28.6-36 -12.6 7.5-26.5 12.9-41.3 15.8 -11.9-12.6-28.8-20.6-47.5-20.6 -42 0-72.9 39.2-63.4 79.9 -54.1-2.7-102.1-28.6-134.2-68 -17 29.2-8.8 67.5 20.1 86.9 -10.7-0.3-20.7-3.3-29.5-8.1 -0.7 30.2 20.9 58.4 52.2 64.6 -9.2 2.5-19.2 3.1-29.4 1.1 8.3 25.9 32.3 44.7 60.8 45.2 -27.4 21.4-61.8 31-96.4 27 28.8 18.5 63 29.2 99.8 29.2 120.8 0 189.1-102.1 185-193.6C399.9 193.1 410.9 181.7 419.6 168.6z" /></svg>
                    </a></li>
            <li><a href="mailto:ChorleyMJ@Cardiff.ac.uk" class="icon email" title="Email"><svg viewBox="0 0 512 512">
                        <path d="M101.3 141.6v228.9h0.3 308.4 0.8V141.6H101.3zM375.7 167.8l-119.7 91.5 -119.6-91.5H375.7zM127.6 194.1l64.1 49.1 -64.1 64.1V194.1zM127.8 344.2l84.9-84.9 43.2 33.1 43-32.9 84.7 84.7L127.8 344.2 127.8 344.2zM384.4 307.8l-64.4-64.4 64.4-49.3V307.8z" /></svg>
                    </a></li>
            <li><a href="https://www.facebook.com/martin.chorley" class="icon facebook" title="Facebook"><svg viewBox="0 0 512 512">
                        <path d="M211.9 197.4h-36.7v59.9h36.7V433.1h70.5V256.5h49.2l5.2-59.1h-54.4c0 0 0-22.1 0-33.7 0-13.9 2.8-19.5 16.3-19.5 10.9 0 38.2 0 38.2 0V82.9c0 0-40.2 0-48.8 0 -52.5 0-76.1 23.1-76.1 67.3C211.9 188.8 211.9 197.4 211.9 197.4z" /></svg>
                    </a></li>
            <li><a href="https://www.github.com/martinjc" class="icon github" title="GitHub"><svg viewBox="0 0 512 512">
                        <path d="M256 70.7c-102.6 0-185.9 83.2-185.9 185.9 0 82.1 53.3 151.8 127.1 176.4 9.3 1.7 12.3-4 12.3-8.9V389.4c-51.7 11.3-62.5-21.9-62.5-21.9 -8.4-21.5-20.6-27.2-20.6-27.2 -16.9-11.5 1.3-11.3 1.3-11.3 18.7 1.3 28.5 19.2 28.5 19.2 16.6 28.4 43.5 20.2 54.1 15.4 1.7-12 6.5-20.2 11.8-24.9 -41.3-4.7-84.7-20.6-84.7-91.9 0-20.3 7.3-36.9 19.2-49.9 -1.9-4.7-8.3-23.6 1.8-49.2 0 0 15.6-5 51.1 19.1 14.8-4.1 30.7-6.2 46.5-6.3 15.8 0.1 31.7 2.1 46.6 6.3 35.5-24 51.1-19.1 51.1-19.1 10.1 25.6 3.8 44.5 1.8 49.2 11.9 13 19.1 29.6 19.1 49.9 0 71.4-43.5 87.1-84.9 91.7 6.7 5.8 12.8 17.1 12.8 34.4 0 24.9 0 44.9 0 51 0 4.9 3 10.7 12.4 8.9 73.8-24.6 127-94.3 127-176.4C441.9 153.9 358.6 70.7 256 70.7z" /></svg>
                    </a></li>
            <li><a href="https://www.instagram.com/martinjam" class="icon instagram" title="Instagram"><svg viewBox="0 0 512 512">
                        <g>
                            <path d="M256 109.3c47.8 0 53.4 0.2 72.3 1 17.4 0.8 26.9 3.7 33.2 6.2 8.4 3.2 14.3 7.1 20.6 13.4 6.3 6.3 10.1 12.2 13.4 20.6 2.5 6.3 5.4 15.8 6.2 33.2 0.9 18.9 1 24.5 1 72.3s-0.2 53.4-1 72.3c-0.8 17.4-3.7 26.9-6.2 33.2 -3.2 8.4-7.1 14.3-13.4 20.6 -6.3 6.3-12.2 10.1-20.6 13.4 -6.3 2.5-15.8 5.4-33.2 6.2 -18.9 0.9-24.5 1-72.3 1s-53.4-0.2-72.3-1c-17.4-0.8-26.9-3.7-33.2-6.2 -8.4-3.2-14.3-7.1-20.6-13.4 -6.3-6.3-10.1-12.2-13.4-20.6 -2.5-6.3-5.4-15.8-6.2-33.2 -0.9-18.9-1-24.5-1-72.3s0.2-53.4 1-72.3c0.8-17.4 3.7-26.9 6.2-33.2 3.2-8.4 7.1-14.3 13.4-20.6 6.3-6.3 12.2-10.1 20.6-13.4 6.3-2.5 15.8-5.4 33.2-6.2C202.6 109.5 208.2 109.3 256 109.3M256 77.1c-48.6 0-54.7 0.2-73.8 1.1 -19 0.9-32.1 3.9-43.4 8.3 -11.8 4.6-21.7 10.7-31.7 20.6 -9.9 9.9-16.1 19.9-20.6 31.7 -4.4 11.4-7.4 24.4-8.3 43.4 -0.9 19.1-1.1 25.2-1.1 73.8 0 48.6 0.2 54.7 1.1 73.8 0.9 19 3.9 32.1 8.3 43.4 4.6 11.8 10.7 21.7 20.6 31.7 9.9 9.9 19.9 16.1 31.7 20.6 11.4 4.4 24.4 7.4 43.4 8.3 19.1 0.9 25.2 1.1 73.8 1.1s54.7-0.2 73.8-1.1c19-0.9 32.1-3.9 43.4-8.3 11.8-4.6 21.7-10.7 31.7-20.6 9.9-9.9 16.1-19.9 20.6-31.7 4.4-11.4 7.4-24.4 8.3-43.4 0.9-19.1 1.1-25.2 1.1-73.8s-0.2-54.7-1.1-73.8c-0.9-19-3.9-32.1-8.3-43.4 -4.6-11.8-10.7-21.7-20.6-31.7 -9.9-9.9-19.9-16.1-31.7-20.6 -11.4-4.4-24.4-7.4-43.4-8.3C310.7 77.3 304.6 77.1 256 77.1L256 77.1z" />
                            <path d="M256 164.1c-50.7 0-91.9 41.1-91.9 91.9s41.1 91.9 91.9 91.9 91.9-41.1 91.9-91.9S306.7 164.1 256 164.1zM256 315.6c-32.9 0-59.6-26.7-59.6-59.6s26.7-59.6 59.6-59.6 59.6 26.7 59.6 59.6S288.9 315.6 256 315.6z" />
                            <circle cx="351.5" cy="160.5" r="21.5" />
                        </g>
                    </svg>
                    </a></li>
            <li><a href="https://www.last.fm/user/takitomouse" class="icon lastfm" title="Last.fm"><svg viewBox="0 0 512 512">
                        <path d="M230.104 336.568l-13.607-36.988c0 0-22.11 24.66-55.268 24.66 -29.341 0-50.172-25.512-50.172-66.328 0-52.293 26.359-71.001 52.297-71.001 37.412 0 49.316 24.234 59.522 55.273l13.607 42.518c13.603 41.236 39.113 74.402 112.666 74.402 52.727 0 88.437-16.155 88.437-58.672 0-34.438-19.56-52.297-56.125-60.802l-27.209-5.951c-18.707-4.252-24.233-11.906-24.233-24.659 0-14.456 11.478-22.96 30.189-22.96 20.406 0 31.458 7.653 33.162 25.935l42.516-5.103c-3.402-38.263-29.761-53.996-73.127-53.996 -38.266 0-75.68 14.456-75.68 60.799 0 28.912 14.029 47.197 49.315 55.697l28.916 6.801c21.683 5.104 28.908 14.031 28.908 26.363 0 15.73-15.305 22.107-44.218 22.107 -42.941 0-60.794-22.534-70.999-53.574l-14.032-42.513c-17.854-55.271-46.342-75.68-102.892-75.68 -62.499-0.001-95.663 39.538-95.663 106.715 0 64.628 33.164 99.489 92.689 99.489C207.141 359.1 230.104 336.568 230.104 336.568L230.104 336.568z" /></svg>
                    </a></li>
            <li><a href="https://uk.linkedin.com/in/martinchorley" class="icon linkedin" title="LinkedIn"><svg viewBox="0 0 512 512">
                        <path d="M186.4 142.4c0 19-15.3 34.5-34.2 34.5 -18.9 0-34.2-15.4-34.2-34.5 0-19 15.3-34.5 34.2-34.5C171.1 107.9 186.4 123.4 186.4 142.4zM181.4 201.3h-57.8V388.1h57.8V201.3zM273.8 201.3h-55.4V388.1h55.4c0 0 0-69.3 0-98 0-26.3 12.1-41.9 35.2-41.9 21.3 0 31.5 15 31.5 41.9 0 26.9 0 98 0 98h57.5c0 0 0-68.2 0-118.3 0-50-28.3-74.2-68-74.2 -39.6 0-56.3 30.9-56.3 30.9v-25.2H273.8z" /></svg>
                    </a></li>
        </ul>
    </div>


    <nav>
        <ul>|
            <li><a href="/">&nbsp;home</a>&nbsp;|</li>
            <li><a href="/blog">&nbsp;blog</a>&nbsp;|</li>
            <li><a href="/teaching">&nbsp;teaching</a>&nbsp;|</li>
            <li><a href="/research">&nbsp;research</a>&nbsp;|</li>
            <li><a href="/publications">&nbsp;publications</a>&nbsp;|</li>
        </ul>
    </nav>
</header><div id="content">
<main>
    

<ul class="pagination">
    
    <li class="page-item">
        <a href="/tags/python/" class="page-link" aria-label="First"><span aria-hidden="true">&laquo;&laquo;</span></a>
    </li>
    
    <li class="page-item disabled">
    <a href="" class="page-link" aria-label="Previous"><span aria-hidden="true">&laquo;</span></a>
    </li>
    
    
    
    
    
    
        
        
    
    
    <li class="page-item active"><a class="page-link" href="/tags/python/">1</a></li>
    
    
    
    
    
    
        
        
    
    
    <li class="page-item"><a class="page-link" href="/tags/python/page/2/">2</a></li>
    
    
    <li class="page-item">
    <a href="/tags/python/page/2/" class="page-link" aria-label="Next"><span aria-hidden="true">&raquo;</span></a>
    </li>
    
    <li class="page-item">
        <a href="/tags/python/page/2/" class="page-link" aria-label="Last"><span aria-hidden="true">&raquo;&raquo;</span></a>
    </li>
    
</ul>

    
    <article class="blogpost">
        <header>
            <h1> <a href="/2016/11/02/scraping-the-assembly/">
                    Scraping the Assembly
                </a></h1>
            <p class="author">
                <span class="date">Wednesday, Nov 2, 2016</span>
            </p>
        </header>
        <div class="content">
            <p>M’colleague is currently teaching a first-semester module on Data Journalism to the students on our MSc in Computational and Data Journalism. As part of this, they need to do some sort of data project. One of the students is looking at the <a href="http://allowances.assembly.wales/Default.aspx?Option=switch">expenses of Welsh Assembly Members</a>. These are all freely available online, but not in an easy to manipulate form. According to the Assembly they’d be happy to give the data out as a spreadsheet, if we submitted an FOI.</p>

<p>To me, this seems quite stupid. The information is all online and freely accessible. You’ve admitted you’re willing to give it out to anyone who submits an FOI. So why not just make the raw data available to download? This does not sound like a helpful <a href="http://www.opengovernment.org.uk/about/">Open Government</a> to me. Anyway, for whatever reason, they’ve chosen not to, and we can’t be bothered to wait around for an FOI to come back. It’s much quicker and easier to build a scraper! We’ll just use selenium to drive a web browser, submit a search, page through all the results collecting the details, then dump it all out to csv. Simple.</p>










<figure>
    <img src="/2016/11/02/scraping-the-assembly/out.gif" width="679" height="510" alt="Scraping AM expenses">
    <figcaption>
        <small>
            Scraping AM expenses
        </small>
    </figcaption>
</figure>

<p>I built this as a quick hack this morning. It took about an hour or so, and it shows. The code is not robust in any way, but it works. You can ask it for data from any year (or a number of years) and it’ll happily sit there churning its way through the results and spitting them out as both .csv and .json.</p>

<p>All the code is <a href="https://github.com/martinjc/assembly-expenses-scraper">available on Github</a> and it’s under an MIT Licence. Have fun 😉</p>

        </div>
    </article>
    
    <article class="blogpost">
        <header>
            <h1> <a href="/2015/02/05/accessing-and-scraping-myfitnesspal-data-with-python/">
                    Accessing and Scraping MyFitnessPal Data with Python
                </a></h1>
            <p class="author">
                <span class="date">Thursday, Feb 5, 2015</span>
            </p>
        </header>
        <div class="content">
            <p>Interesting news this morning that MyFitnessPal has been bought by Under Armour for  $475 million. I&rsquo;ve used MFP for many years now, and it was pretty helpful in helping me lose all the excess PhD weight that I&rsquo;d put on, and then maintaining a healthy(ish) lifestyle since 2010.</p>

<p>News of an acquisition always has me slightly worried though - not for someone else having access to my data, as I&rsquo;ve made my peace with the fact that using a free service generally means that it&rsquo;s me that&rsquo;s being sold. Giving away my data is the cost of doing business. Rather, it worries me that I may lose access to all the data I&rsquo;ve collected. I have no idea what Under Armour intend for the service in the long run, and while its likely that MFP will continue with business as usual for the foreseeable, it&rsquo;s always worth having a backup of your data.</p>

<p><a href="/2011/06/09/logging-in-to-websites-with-python/">A few years ago, I wrote a couple of python scripts</a> to back up data from MFP and then extract the food and exercise info from the raw HTML. These scripts use Python and Beautiful Soup to do a login to MFP, then go back through your diary history and save all the raw HTML pages, essentially scraping your data.</p>

<p>I came to run them this morning and found they needed a couple of changes to deal with site updates. I&rsquo;ve made the necessary updates and the full code for all the scripts is <a href="https://github.com/martinjc/mfp-scraper">available on GitHub</a>. It&rsquo;s not great, but it works. The code is Python 2 and requires BeautifulSoup and Matplotlib (if you want to use generate_plots.py).</p>

        </div>
    </article>
    
    <article class="blogpost">
        <header>
            <h1> <a href="/2015/01/05/compj-labs-postcodes/">
                    CompJ Labs - Postcodes
                </a></h1>
            <p class="author">
                <span class="date">Monday, Jan 5, 2015</span>
            </p>
        </header>
        <div class="content">
            <p>I&rsquo;ve thrown up a <a href="http://compj.cs.cf.ac.uk/2015/01/05/empty-properties-postcodes/">post</a> on <a href="http://compj.cs.cf.ac.uk">compj.cs.cf.ac.uk</a> about some work we&rsquo;ve done looking at empty properties in Cardiff. I&rsquo;ll add further posts over the coming weeks about other data analysis we&rsquo;ve done on this topic, and new work looking at the Welsh Assembly.</p>

        </div>
    </article>
    
    <article class="blogpost">
        <header>
            <h1> <a href="/2014/11/19/quick-and-dirty-twitter-api-in-python/">
                    Quick and Dirty Twitter API in Python
                </a></h1>
            <p class="author">
                <span class="date">Wednesday, Nov 19, 2014</span>
            </p>
        </header>
        <div class="content">
            <p><strong>QUICK DISCLAIMER: this is a quick and dirty solution to a problem, so may not represent best coding practice, and has absolutely no error checking or handling. Use with caution&hellip;</strong></p>

<p>A recent project has needed me to scrape some data from Twitter. I considered using <a href="https://github.com/tweepy/tweepy">Tweepy</a>, but as it was a project for the <a href="compj.cs.cf.ac.uk">MSc in Computational Journalism</a>, I thought it would be more interesting to write our own simple Twitter API wrapper in Python.</p>

<p>The code presented here will allow you to make any API request to Twitter that uses a GET request, so is really only useful for getting data <em>from</em> Twitter, not sending it <em>to</em> Twitter. It is also only for using with the REST API, not the streaming API, so if you&rsquo;re looking for realtime monitoring, this is not the API wrapper you&rsquo;re looking for. This API wrapper also uses a single user&rsquo;s authentication (yours), so is not setup to allow other users to use Twitter through your application.</p>

<p>The first step is to get some access credentials from Twitter. Head over to <a href="https://apps.twitter.com/">https://apps.twitter.com/</a> and register a new application. Once the application is created, you&rsquo;ll be able to access its details. Under &lsquo;Keys and Access Tokens&rsquo; are four values we&rsquo;re going to need for the API - the  <code>Consumer Key</code> and <code>Consumer Secret</code>, and the <code>Access Token</code> and <code>Access Token Secret</code>. Copy all four values into a new python file, and save it as &lsquo;<code>_credentials.py</code>&rsquo;. Once we have the credentials, we can write some code to make some API requests!</p>

<p>First, we define a Twitter API object that will carry out our API requests. We need to store the API url, and some details to allow us to throttle our requests to Twitter to fit inside their rate limiting.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Twitter_API</span>:

 <span style="color:#66d9ef">def</span> __init__(self):

   <span style="color:#75715e"># URL for accessing API</span>
   scheme <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;https://&#34;</span>
   api_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;api.twitter.com&#34;</span>
   version <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;1.1&#34;</span>

   self<span style="color:#f92672">.</span>api_base <span style="color:#f92672">=</span> scheme <span style="color:#f92672">+</span> api_url <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;/&#34;</span> <span style="color:#f92672">+</span> version

   <span style="color:#75715e">#</span>
   <span style="color:#75715e"># seconds between queries to each endpoint</span>
   <span style="color:#75715e"># queries in this project limited to 180</span>
   <span style="color:#75715e"># per 15 minutes</span>
   query_interval <span style="color:#f92672">=</span> float(<span style="color:#ae81ff">15</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">60</span>)<span style="color:#f92672">/</span>(<span style="color:#ae81ff">175</span>)

   <span style="color:#75715e">#</span>
   <span style="color:#75715e"># rate limiting timer</span>
   self<span style="color:#f92672">.</span>__monitor <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;wait&#39;</span>:query_interval,
     <span style="color:#e6db74">&#39;earliest&#39;</span>:None,
     <span style="color:#e6db74">&#39;timer&#39;</span>:None}</code></pre></div>

<p>We add a rate limiting method that will make our API sleep if we are requesting things from Twitter too fast:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#</span>
<span style="color:#75715e"># rate_controller puts the thread to sleep</span>
<span style="color:#75715e"># if we&#39;re hitting the API too fast</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__rate_controller</span>(self, monitor_dict):

 <span style="color:#75715e">#</span>
 <span style="color:#75715e"># join the timer thread</span>
 <span style="color:#66d9ef">if</span> monitor_dict[<span style="color:#e6db74">&#39;timer&#39;</span>] <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
 monitor_dict[<span style="color:#e6db74">&#39;timer&#39;</span>]<span style="color:#f92672">.</span>join()

 <span style="color:#75715e"># sleep if necessary</span>
 <span style="color:#66d9ef">while</span> time<span style="color:#f92672">.</span>time() <span style="color:#f92672">&lt;</span> monitor_dict[<span style="color:#e6db74">&#39;earliest&#39;</span>]:
   time<span style="color:#f92672">.</span>sleep(monitor_dict[<span style="color:#e6db74">&#39;earliest&#39;</span>] <span style="color:#f92672">-</span> time<span style="color:#f92672">.</span>time())

 <span style="color:#75715e"># work out then the next API call can be made</span>
 earliest <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time() <span style="color:#f92672">+</span> monitor_dict[<span style="color:#e6db74">&#39;wait&#39;</span>]
 timer <span style="color:#f92672">=</span> threading<span style="color:#f92672">.</span>Timer( earliest<span style="color:#f92672">-</span>time<span style="color:#f92672">.</span>time(), <span style="color:#66d9ef">lambda</span>: None )
 monitor_dict[<span style="color:#e6db74">&#39;earliest&#39;</span>] <span style="color:#f92672">=</span> earliest
 monitor_dict[<span style="color:#e6db74">&#39;timer&#39;</span>] <span style="color:#f92672">=</span> timer
 monitor_dict[<span style="color:#e6db74">&#39;timer&#39;</span>]<span style="color:#f92672">.</span>start()</code></pre></div>

<p>The Twitter API requires us to supply authentication headers in the request. One of these headers is a signature, created by encoding details of the request. We can write a function that will take in all the details of the request (method, url, parameters) and create the signature:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#</span>
<span style="color:#75715e"># make the signature for the API request</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_signature</span>(self, method, url, params):

 <span style="color:#75715e"># escape special characters in all parameter keys</span>
 encoded_params <span style="color:#f92672">=</span> {}
 <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> params<span style="color:#f92672">.</span>items():
   encoded_k <span style="color:#f92672">=</span> urllib<span style="color:#f92672">.</span>parse<span style="color:#f92672">.</span>quote_plus(str(k))
   encoded_v <span style="color:#f92672">=</span> urllib<span style="color:#f92672">.</span>parse<span style="color:#f92672">.</span>quote_plus(str(v))
   encoded_params[encoded_k] <span style="color:#f92672">=</span> encoded_v

 <span style="color:#75715e"># sort the parameters alphabetically by key</span>
 sorted_keys <span style="color:#f92672">=</span> sorted(encoded_params<span style="color:#f92672">.</span>keys())

 <span style="color:#75715e"># create a string from the parameters</span>
 signing_string <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>

 count <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
 <span style="color:#66d9ef">for</span> key <span style="color:#f92672">in</span> sorted_keys:
   signing_string <span style="color:#f92672">+=</span> key
   signing_string <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;=&#34;</span>
   signing_string <span style="color:#f92672">+=</span> encoded_params[key]
   count <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
   <span style="color:#66d9ef">if</span> count <span style="color:#f92672">&lt;</span> len(sorted_keys):
     signing_string <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;&amp;&#34;</span>

 <span style="color:#75715e"># construct the base string</span>
 base_string <span style="color:#f92672">=</span> method<span style="color:#f92672">.</span>upper()
 base_string <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;&amp;&#34;</span>
 base_string <span style="color:#f92672">+=</span> urllib<span style="color:#f92672">.</span>parse<span style="color:#f92672">.</span>quote_plus(url)
 base_string <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;&amp;&#34;</span>
 base_string <span style="color:#f92672">+=</span> urllib<span style="color:#f92672">.</span>parse<span style="color:#f92672">.</span>quote_plus(signing_string)

 <span style="color:#75715e"># construct the key</span>
 signing_key <span style="color:#f92672">=</span> urllib<span style="color:#f92672">.</span>parse<span style="color:#f92672">.</span>quote_plus(client_secret) <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;&amp;&#34;</span> <span style="color:#f92672">+</span> urllib<span style="color:#f92672">.</span>parse<span style="color:#f92672">.</span>quote_plus(access_secret)

 <span style="color:#75715e"># encrypt the base string with the key, and base64 encode the result</span>
 hashed <span style="color:#f92672">=</span> hmac<span style="color:#f92672">.</span>new(signing_key<span style="color:#f92672">.</span>encode(), base_string<span style="color:#f92672">.</span>encode(), sha1)
 signature <span style="color:#f92672">=</span> base64<span style="color:#f92672">.</span>b64encode(hashed<span style="color:#f92672">.</span>digest())
 <span style="color:#66d9ef">return</span> signature<span style="color:#f92672">.</span>decode(<span style="color:#e6db74">&#34;utf-8&#34;</span>)</code></pre></div>

<p>Finally, we can write a method to actually <em>make</em> the API request:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">query_get</span>(self, endpoint, aspect, get_params<span style="color:#f92672">=</span>{}):

 <span style="color:#75715e">#</span>
 <span style="color:#75715e"># rate limiting</span>
 self<span style="color:#f92672">.</span>__rate_controller(self<span style="color:#f92672">.</span>__monitor)

 <span style="color:#75715e"># ensure we&#39;re dealing with strings as parameters</span>
 str_param_data <span style="color:#f92672">=</span> {}
 <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> get_params<span style="color:#f92672">.</span>items():
   str_param_data[str(k)] <span style="color:#f92672">=</span> str(v)

 <span style="color:#75715e"># construct the query url</span>
 url <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>api_base <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;/&#34;</span> <span style="color:#f92672">+</span> endpoint <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;/&#34;</span> <span style="color:#f92672">+</span> aspect <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;.json&#34;</span>

 <span style="color:#75715e"># add the header parameters for authorisation</span>
 header_parameters <span style="color:#f92672">=</span> {
   <span style="color:#e6db74">&#34;oauth_consumer_key&#34;</span>: client_id,
   <span style="color:#e6db74">&#34;oauth_nonce&#34;</span>: uuid<span style="color:#f92672">.</span>uuid4(),
   <span style="color:#e6db74">&#34;oauth_signature_method&#34;</span>: <span style="color:#e6db74">&#34;HMAC-SHA1&#34;</span>,
   <span style="color:#e6db74">&#34;oauth_timestamp&#34;</span>: time<span style="color:#f92672">.</span>time(),
   <span style="color:#e6db74">&#34;oauth_token&#34;</span>: access_token,
   <span style="color:#e6db74">&#34;oauth_version&#34;</span>: <span style="color:#ae81ff">1.0</span>
 }

 <span style="color:#75715e"># collect all the parameters together for creating the signature</span>
 signing_parameters <span style="color:#f92672">=</span> {}
 <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> header_parameters<span style="color:#f92672">.</span>items():
   signing_parameters[k] <span style="color:#f92672">=</span> v
 <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> str_param_data<span style="color:#f92672">.</span>items():
   signing_parameters[k] <span style="color:#f92672">=</span> v

 <span style="color:#75715e"># create the signature and add it to the header parameters</span>
 header_parameters[<span style="color:#e6db74">&#34;oauth_signature&#34;</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>get_signature(<span style="color:#e6db74">&#34;GET&#34;</span>, url, signing_parameters)

 <span style="color:#75715e"># add the OAuth headers</span>
 header_string <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;OAuth &#34;</span>
 count <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
 <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> header_parameters<span style="color:#f92672">.</span>items():
   header_string <span style="color:#f92672">+=</span> urllib<span style="color:#f92672">.</span>parse<span style="color:#f92672">.</span>quote_plus(str(k))
   header_string <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;=</span><span style="color:#ae81ff">\&#34;</span><span style="color:#e6db74">&#34;</span>
   header_string <span style="color:#f92672">+=</span> urllib<span style="color:#f92672">.</span>parse<span style="color:#f92672">.</span>quote_plus(str(v))
   header_string <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\&#34;</span><span style="color:#e6db74">&#34;</span>
   count <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
   <span style="color:#66d9ef">if</span> count <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">7</span>:
     header_string <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;, &#34;</span>

 headers <span style="color:#f92672">=</span> {
   <span style="color:#e6db74">&#34;Authorization&#34;</span>: header_string
 }

 <span style="color:#75715e"># create the full url including parameters</span>
 url <span style="color:#f92672">=</span> url <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;?&#34;</span> <span style="color:#f92672">+</span> urllib<span style="color:#f92672">.</span>parse<span style="color:#f92672">.</span>urlencode(str_param_data)
 request <span style="color:#f92672">=</span> urllib<span style="color:#f92672">.</span>request<span style="color:#f92672">.</span>Request(url, headers<span style="color:#f92672">=</span>headers)

 <span style="color:#75715e"># make the API request</span>
 <span style="color:#66d9ef">try</span>:
   response <span style="color:#f92672">=</span> urllib<span style="color:#f92672">.</span>request<span style="color:#f92672">.</span>urlopen(request)
   <span style="color:#66d9ef">except</span> urllib<span style="color:#f92672">.</span>error<span style="color:#f92672">.</span>HTTPError <span style="color:#66d9ef">as</span> e:
   <span style="color:#66d9ef">print</span>(e)
 <span style="color:#66d9ef">raise</span> e
   <span style="color:#66d9ef">except</span> urllib<span style="color:#f92672">.</span>error<span style="color:#f92672">.</span>URLError <span style="color:#66d9ef">as</span> e:
   <span style="color:#66d9ef">print</span>(e)
   <span style="color:#66d9ef">raise</span> e

 <span style="color:#75715e"># read the response and return the json</span>
 raw_data <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>read()<span style="color:#f92672">.</span>decode(<span style="color:#e6db74">&#34;utf-8&#34;</span>)
 <span style="color:#66d9ef">return</span> json<span style="color:#f92672">.</span>loads(raw_data)</code></pre></div>

<p>Putting this all together, we have a simple Python class that acts as an API wrapper for GET requests to the Twitter REST API, including the signing and authentication of those requests. Using it is as simple as:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"> ta <span style="color:#f92672">=</span> Twitter_API()

 <span style="color:#75715e"># retrieve tweets for a user</span>
 params <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#34;screen_name&#34;</span>: <span style="color:#e6db74">&#34;martinjc&#34;</span>,
 }

 user_tweets <span style="color:#f92672">=</span> ta<span style="color:#f92672">.</span>query_get(<span style="color:#e6db74">&#34;statuses&#34;</span>, <span style="color:#e6db74">&#34;user_timeline&#34;</span>, params)</code></pre></div>

<p>As always, the full code is online on Github, in both my <a href="https://github.com/CompJCDF/Simple-Python-Twitter-API">personal account</a> and the <a href="https://github.com/CompJCDF">account for</a> the <a href="http://compj.cs.cf.ac.uk/">MSc Computational Journalism</a>.</p>

        </div>
    </article>
    
    <article class="blogpost">
        <header>
            <h1> <a href="/2014/09/15/ccgs-and-wpcs-via-the-medium-of-oas/">
                    CCGs and WPCs via the medium of OAs
                </a></h1>
            <p class="author">
                <span class="date">Monday, Sep 15, 2014</span>
            </p>
        </header>
        <div class="content">
            <p>As I was eating lunch this afternoon, I spotted a conversation between <a href="https://twitter.com/joereddington">@JoeReddington</a> and <a href="https://twitter.com/mysociety">@MySociety</a> whizz past in Tweetdeck. I traced the conversation back to the beginning and found this request for data:</p>

<p><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I&#39;m looking for a dataset that shows which UK constituencies are in which NHS trust areas.  Anyone?</p>&mdash; retired account (@retired10072015) <a href="https://twitter.com/retired10072015/status/511105946710716416">September 14, 2014</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>I&rsquo;ve been doing a lot of playing with geographic data recently while preparing to release a site making it easier to get GeoJSON boundaries of various areas in the UK. As a result, I&rsquo;ve become pretty familiar with the <a href="https://geoportal.statistics.gov.uk/geoportal/catalog/search/browse/browse.page">Office of National Statistics Geography portal</a>, and the data available there. I figured it must be pretty simple to hack something together to provide the data Joseph was looking for, so I took a few minutes out of lunch to see if I could help.</p>

<p>Checking the lookup tables at the ONS, it was clear that unfortunately there was no simple &lsquo;NHS Trust to Parliamentary Constituency&rsquo; lookup table. However, there were two separate lookups involving Output Areas (OAs). One allows you to lookup <a href="https://geoportal.statistics.gov.uk/geoportal/catalog/search/resource/details.page?uuid=%7B441E0CBF-1421-4BF5-BBC9-5B7C0EA0FE44%7D">which Parliamentary Constituency (WPC) an OA belongs to</a>. The other allows you to lookup <a href="https://geoportal.statistics.gov.uk/geoportal/catalog/search/resource/details.page?uuid=%7B15C3A07F-F5E1-4CE8-9CFD-24B3589C725B%7D">which NHS Clinical Commissioning Group (CCG) an OA belongs to</a>. Clearly, all that&rsquo;s required to link the two together is a bit of quick scripting to tie them both together via the Output Areas.</p>

<p>First, let&rsquo;s create a dictionary with an entry for each CCG. For each CCG we&rsquo;ll store it&rsquo;s ID, name, and a set of OAs contained within. We&rsquo;ll also add  an empty set for the WPCs contained within the CCG:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> csv
<span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> defaultdict

data <span style="color:#f92672">=</span> {}

<span style="color:#75715e"># extract information about clinical commissioning groups</span>
<span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;OA11_CCG13_NHSAT_NHSCR_EN_LU.csv&#39;</span>, <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> oa_to_cgc_file:
  reader <span style="color:#f92672">=</span> csv<span style="color:#f92672">.</span>DictReader(oa_to_cgc_file)
  <span style="color:#66d9ef">for</span> row <span style="color:#f92672">in</span> reader:
    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> data<span style="color:#f92672">.</span>get(row[<span style="color:#e6db74">&#39;CCG13CD&#39;</span>]):
      data[row[<span style="color:#e6db74">&#39;CCG13CD&#39;</span>]] <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;CCG13CD&#39;</span>: row[<span style="color:#e6db74">&#39;CCG13CD&#39;</span>], <span style="color:#e6db74">&#39;CCG13NM&#39;</span>: row[<span style="color:#e6db74">&#39;CCG13NM&#39;</span>], <span style="color:#e6db74">&#39;PCON11CD list&#39;</span>: set(), <span style="color:#e6db74">&#39;PCON11NM list&#39;</span>: set(), <span style="color:#e6db74">&#39;OA11CD list&#39;</span>: set(),}
    data[row[<span style="color:#e6db74">&#39;CCG13CD&#39;</span>]][<span style="color:#e6db74">&#39;OA11CD list&#39;</span>]<span style="color:#f92672">.</span>add(row[<span style="color:#e6db74">&#39;OA11CD&#39;</span>])</code></pre></div>

<p>Next we create a lookup table that allows us to convert from OA to WPC:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># extract information for output area to constituency lookup</span>
oas <span style="color:#f92672">=</span> {}
pcon_nm <span style="color:#f92672">=</span> {}

<span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;OA11_PCON11_EER11_EW_LU.csv&#39;</span>, <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> oa_to_pcon_file:
  reader <span style="color:#f92672">=</span> csv<span style="color:#f92672">.</span>DictReader(oa_to_pcon_file)
  <span style="color:#66d9ef">for</span> row <span style="color:#f92672">in</span> reader:
    oas[row[<span style="color:#e6db74">&#39;OA11CD&#39;</span>]] <span style="color:#f92672">=</span> row[<span style="color:#e6db74">&#39;PCON11CD&#39;</span>]
    pcon_nm[row[<span style="color:#e6db74">&#39;PCON11CD&#39;</span>]] <span style="color:#f92672">=</span> row[<span style="color:#e6db74">&#39;PCON11NM&#39;</span>]</code></pre></div>

<p>As the almost last step we go through the CCGs, and for each one we go through the list of OAs it covers, and lookup the WPC each OA belongs to:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># go through all the ccgs and lookup pcons from oas</span>
<span style="color:#66d9ef">for</span> ccg, d <span style="color:#f92672">in</span> data<span style="color:#f92672">.</span>iteritems():

 <span style="color:#66d9ef">for</span> oa <span style="color:#f92672">in</span> d[<span style="color:#e6db74">&#39;OA11CD list&#39;</span>]:
   d[<span style="color:#e6db74">&#39;PCON11CD list&#39;</span>]<span style="color:#f92672">.</span>add(oas[oa])
   d[<span style="color:#e6db74">&#39;PCON11NM list&#39;</span>]<span style="color:#f92672">.</span>add(pcon_nm[oas[oa]])

<span style="color:#66d9ef">del</span> d[<span style="color:#e6db74">&#39;OA11CD list&#39;</span>]</code></pre></div>

<p>Finally we just need to output the data:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#66d9ef">for</span> d <span style="color:#f92672">in</span> data<span style="color:#f92672">.</span>values():

     d[<span style="color:#e6db74">&#39;PCON11CD list&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;;&#39;</span><span style="color:#f92672">.</span>join(d[<span style="color:#e6db74">&#39;PCON11CD list&#39;</span>])
     d[<span style="color:#e6db74">&#39;PCON11NM list&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;;&#39;</span><span style="color:#f92672">.</span>join(d[<span style="color:#e6db74">&#39;PCON11NM list&#39;</span>])

    <span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;output.csv&#39;</span>, <span style="color:#e6db74">&#39;w&#39;</span>) <span style="color:#66d9ef">as</span> out_file:
      writer <span style="color:#f92672">=</span> csv<span style="color:#f92672">.</span>DictWriter(out_file, [<span style="color:#e6db74">&#39;CCG13CD&#39;</span>, <span style="color:#e6db74">&#39;CCG13NM&#39;</span>, <span style="color:#e6db74">&#39;PCON11CD list&#39;</span>, <span style="color:#e6db74">&#39;PCON11NM list&#39;</span>])
      writer<span style="color:#f92672">.</span>writeheader()
      writer<span style="color:#f92672">.</span>writerows(data<span style="color:#f92672">.</span>values())</code></pre></div>

<p>Run the script, and we get a nice CSV with one row for each CCG, each row containing a list of the WPC ids and names the CCG covers.</p>

<p>Of course, this data only covers England (as CCGs are a division in NHS England). Although there don&rsquo;t seem to be lookups for OAs to Health Boards in Scotland, or from OAs to Local Health Boards in Wales, it should still be possible to do something similar for these countries using Parliamentary Wards as the intermediate geography, as lookups for Wards to Health Boards and Local Health Boards are available. It&rsquo;s also not immediately clear how well the boundaries for CCGs and WPCs match up, that would require further investigation, depending on what the lookup is to be used for.</p>

<p>All the code, input and output for this task is available on <a href="https://github.com/martinjc/CCG--Constituencies">my github page</a>.</p>

        </div>
    </article>
    
    <article class="blogpost">
        <header>
            <h1> <a href="/2014/08/01/foursquare-icon-downloading-yet-again/">
                    Foursquare icon downloading (yet again)
                </a></h1>
            <p class="author">
                <span class="date">Friday, Aug 1, 2014</span>
            </p>
        </header>
        <div class="content">
            <p><a href="/2011/10/01/colourful-foursquare-category-icons/">Previously</a> I&rsquo;ve<a href="/2012/01/31/foursquare-category-icon-downloader-2/"> written</a> about a little script to download all the category icons from Foursquare and to create many different coloured versions of them. I&rsquo;ve recently had to do this again for a project, and found my previous script did not work with a recent API version. I&rsquo;ve updated the script to fix it and put it up on <a href="https://github.com/martinjc/foursquare_icons">github</a>.</p>

        </div>
    </article>
    
    <article class="blogpost">
        <header>
            <h1> <a href="/2013/10/25/python-oauth/">
                    Python &#43; OAuth
                </a></h1>
            <p class="author">
                <span class="date">Friday, Oct 25, 2013</span>
            </p>
        </header>
        <div class="content">
            <p>As part of a current project I had the misfortune of having to to deal with a bunch of OAuth authenticated web services using a command line script in Python. Usually this isn&rsquo;t really a problem as most decent client libraries for services such as <a href="https://github.com/tweepy/tweepy">Twitter</a> or <a href="https://pypi.python.org/pypi/foursquare">Foursquare</a> can handle the authentication requests themselves, usually wrapping their own internal OAuth implementation. However, when it comes to web services that don&rsquo;t have existing python client libraries, you have to do the implementation yourself. Unfortunately support for OAuth in Python is a mess, so this is not the most pleasant of tasks, especially when most stackoverflow <a href="http://stackoverflow.com/questions/12628246/how-to-send-oauth-request-with-python-oauth2">posts</a> <a href="http://stackoverflow.com/questions/15610749/github-api-v3-access-via-python-oauth2-library-redirect-issue">on</a> <a href="http://stackoverflow.com/questions/1666415/python-oauth-library">the</a> topic point to massively <a href="http://code.daaku.org/python-oauth/">outdated</a> and <a href="https://github.com/simplegeo/python-oauth2">unmaintained</a> Python libraries.</p>

<p>Fortunately after some digging around, I was able to find a nice, well maintained and <a href="https://rauth.readthedocs.org/en/latest/">fairly well documented</a> solution: <a href="https://github.com/litl/rauth">rauth</a>, which is very clean and easy to use. As an example, I was trying to connect to the Fitbit API, and it really was as simple as following their example.</p>

<p>Firstly, we create an OAuth1Service:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> rauth
<span style="color:#f92672">from</span> _credentials <span style="color:#f92672">import</span> consumer_key, consumer_secret

base_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;https://api.fitbit.com&#34;</span>
request_token_url <span style="color:#f92672">=</span> base_url <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;/oauth/request_token&#34;</span>
access_token_url <span style="color:#f92672">=</span> base_url <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;/oauth/access_token&#34;</span>
authorize_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://www.fitbit.com/oauth/authorize&#34;</span>

fitbit <span style="color:#f92672">=</span> rauth<span style="color:#f92672">.</span>OAuth1Service(
 name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;fitbit&#34;</span>,
 consumer_key<span style="color:#f92672">=</span>consumer_key,
 consumer_secret<span style="color:#f92672">=</span>consumer_secret,
 request_token_url<span style="color:#f92672">=</span>request_token_url,
 access_token_url<span style="color:#f92672">=</span>access_token_url,
 authorize_url<span style="color:#f92672">=</span>authorize_url,
 base_url<span style="color:#f92672">=</span>base_url)</code></pre></div>

<p>Then we get the temporary request token credentials:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">request_token, request_token_secret <span style="color:#f92672">=</span> fitbit<span style="color:#f92672">.</span>get_request_token()

<span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34; request_token = </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> request_token
<span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34; request_token_secret = </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> request_token_secret
<span style="color:#66d9ef">print</span></code></pre></div>

<p>We then ask the user to authorise our application, and give us the PIN so we can prove to the service that they authorised us:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">authorize_url <span style="color:#f92672">=</span> fitbit<span style="color:#f92672">.</span>get_authorize_url(request_token)

<span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34;Go to the following page in your browser: &#34;</span> <span style="color:#f92672">+</span> authorize_url
<span style="color:#66d9ef">print</span>

accepted <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;n&#39;</span>
<span style="color:#66d9ef">while</span> accepted<span style="color:#f92672">.</span>lower() <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;n&#39;</span>:
 accepted <span style="color:#f92672">=</span> raw_input(<span style="color:#e6db74">&#39;Have you authorized me? (y/n) &#39;</span>)
pin <span style="color:#f92672">=</span> raw_input(<span style="color:#e6db74">&#39;Enter PIN from browser &#39;</span>)</code></pre></div>

<p>Finally, we can create an authenticated session and access user data from the service:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">session <span style="color:#f92672">=</span> fitbit<span style="color:#f92672">.</span>get_auth_session(request_token,
 request_token_secret,
 method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;POST&#34;</span>,
 data<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;oauth_verifier&#39;</span>: pin})

<span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34;&#34;</span>
<span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34; access_token = </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> session<span style="color:#f92672">.</span>access_token
<span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34; access_token_secret = </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> session<span style="color:#f92672">.</span>access_token_secret
<span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34;&#34;</span>

url <span style="color:#f92672">=</span> base_url <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;/1/&#34;</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;user/-/profile.json&#34;</span>

r <span style="color:#f92672">=</span> session<span style="color:#f92672">.</span>get(url, params<span style="color:#f92672">=</span>{}, header_auth<span style="color:#f92672">=</span>True)
<span style="color:#66d9ef">print</span> r<span style="color:#f92672">.</span>json()</code></pre></div>

<p>It really is that easy to perform a 3-legged OAuth authentication on the command line. If you&rsquo;re only interested in data from 1 user, and you want to run the app multiple times, once you have the access token and secret, there&rsquo;s nothing to stop you just storing those and re-creating your session each time without having to re-authenticate (assuming the service does not expire access tokens):</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">base_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;https://api.fitbit.com/&#34;</span>
api_version <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;1/&#34;</span>
token <span style="color:#f92672">=</span> (fitbit_oauth_token, fitbit_oauth_secret)
consumer <span style="color:#f92672">=</span> (fitbit_consumer_key, fitbit_consumer_secret)

session <span style="color:#f92672">=</span> rauth<span style="color:#f92672">.</span>OAuth1Session(consumer[<span style="color:#ae81ff">0</span>], consumer[<span style="color:#ae81ff">1</span>], token[<span style="color:#ae81ff">0</span>], token[<span style="color:#ae81ff">1</span>])
url <span style="color:#f92672">=</span> base_url <span style="color:#f92672">+</span> api_version <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;user/-/profile.json&#34;</span>
r <span style="color:#f92672">=</span> session<span style="color:#f92672">.</span>get(url, params<span style="color:#f92672">=</span>{}, header_auth<span style="color:#f92672">=</span>True)
<span style="color:#66d9ef">print</span> r<span style="color:#f92672">.</span>json()</code></pre></div>

<p>So there we have it. Simple OAuth authentication on the command line, in Python. As always, the code is available on <a href="https://github.com/martinjc/rauth---fitbit-example">github</a> if you&rsquo;re interested.</p>

        </div>
    </article>
    
    <article class="blogpost">
        <header>
            <h1> <a href="/2013/08/20/not-another-bloody-wordle/">
                    not another bloody wordle?!?!
                </a></h1>
            <p class="author">
                <span class="date">Tuesday, Aug 20, 2013</span>
            </p>
        </header>
        <div class="content">
            <p>(UPDATE: an earlier version of this was totally wrong. It&rsquo;s better now.)</p>

<p>Inspired by a Facebook post from a colleague, I decided to waste ten minutes this week knocking together a word cloud from the text of my thesis. The process was pretty straightforward.</p>

<p>First up - extracting the text from the thesis. Like all good scienticians, my thesis was written in LaTeX. I thought I could have used a couple of different tools to extract the plain text from the raw .tex input files, but actually none of the tools available from a quick googling seemed to work properly, so I went with extracting the text from the pdf file instead. Fortunately on Mac OS X this is pretty simple, as you can create a straightforward Automator application to extract the text from any pdf file, as documented in step 2 <a href="http://craiccomputing.blogspot.co.uk/2010/11/extracting-text-from-pdf-documents-on.html">here</a>.</p>

<p>Once I had the plain text contents of my thesis in a text file it was just a simple few lines of python (using the excellent <a href="http://nltk.org/">NLTK</a>) to get a frequency distribution of the words in my thesis:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> nltk.probability <span style="color:#f92672">import</span> FreqDist
<span style="color:#f92672">from</span> nltk.tokenize <span style="color:#f92672">import</span> word_tokenize, sent_tokenize

fdist <span style="color:#f92672">=</span> FreqDist()
<span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;2012chorleymjphd.txt&#34;</span>, <span style="color:#e6db74">&#34;r&#34;</span>) <span style="color:#66d9ef">as</span> inputfile:
    <span style="color:#66d9ef">for</span> sentence <span style="color:#f92672">in</span> sent_tokenize(inputfile<span style="color:#f92672">.</span>read()):
        <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> word_tokenize(sentence):
            fdist<span style="color:#f92672">.</span>inc(word<span style="color:#f92672">.</span>lower())

    <span style="color:#66d9ef">for</span> word, count <span style="color:#f92672">in</span> fdist<span style="color:#f92672">.</span>iteritems():
        <span style="color:#66d9ef">if</span> count <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">10</span>:
            <span style="color:#66d9ef">print</span> <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">: </span><span style="color:#e6db74">%d</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (word, count)</code></pre></div>

<p>Then it was just a matter of copying and pasting the word frequency distribution into <a href="http://www.wordle.net/">wordle</a>:</p>

<p><img src="/img/{{ page.date | date: &quot;%Y-%m-%d&quot;}}-{{page.slug}}/thesis-wordle-2.png" alt="Thesis Wordle" /></p>

<p>And there we have it. A not particularly informative but quite nice looking representation of my thesis. As you can guess from the cloud, it&rsquo;s not the most exciting thesis in the world. Interestingly, the word error doesn&rsquo;t seem to be there ;-).</p>

        </div>
    </article>
    
    <article class="blogpost">
        <header>
            <h1> <a href="/2013/08/18/swn-festival-2013-plans-part-1-the-data-2/">
                    SWN Festival 2013 plans – part 1: the data (2!)
                </a></h1>
            <p class="author">
                <span class="date">Sunday, Aug 18, 2013</span>
            </p>
        </header>
        <div class="content">
            <p>In the previous post, I used python and BeautifulSoup to grab the list of artists appearing at <a href="http://www.swnfest.com">SWN Festival 2013</a>, and to scrape their associated soundcloud/twitter/facebook/youtube links (where available).</p>

<p>However, there are more places to find music online than just those listed on the festival site, and some of those extra sources include additional data that I want to collect, so now we need to search these other sources for the artists. Firstly, we need to load the artist data we previously extracted from the festival website, and iterate through the list of artists one by one:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">artists <span style="color:#f92672">=</span> {}
<span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;bands.json&#34;</span>) <span style="color:#66d9ef">as</span> infile:
    artists <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>load(infile)

<span style="color:#66d9ef">for</span> artist, artist_data <span style="color:#f92672">in</span> artists<span style="color:#f92672">.</span>iteritems():</code></pre></div>

<p>The first thing I want to do for each artist it to search Spotify to see if they have any music available there. Spotify has a simple web <a href="https://developer.spotify.com/technologies/web-api/">API</a> for searching which is pretty straightforward to use:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">params <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#34;q&#34;</span> : <span style="color:#e6db74">&#34;artist:&#34;</span> <span style="color:#f92672">+</span> artist<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#34;utf-8&#34;</span>)
}

spotify_root_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://ws.spotify.com/search/1/artist.json&#34;</span>
spotify_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">?</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (spotify_root_url, urllib<span style="color:#f92672">.</span>urlencode(params))

data <span style="color:#f92672">=</span> retrieve_json_data(spotify_url)

<span style="color:#66d9ef">if</span> data<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;artists&#34;</span>, None) <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
    <span style="color:#66d9ef">if</span> len(data[<span style="color:#e6db74">&#34;artists&#34;</span>]) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
        artist_id <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#34;artists&#34;</span>][<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#34;href&#34;</span>]<span style="color:#f92672">.</span>lstrip(<span style="color:#e6db74">&#34;spotify:artist:&#34;</span>)
        artist_data[<span style="color:#e6db74">&#34;spotify_id&#34;</span>] <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#34;artists&#34;</span>][<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#34;href&#34;</span>]
        artist_data[<span style="color:#e6db74">&#34;spotify_url&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://open.spotify.com/artist/&#34;</span> <span style="color:#f92672">+</span> artist_id</code></pre></div>

<p>The &lsquo;retrieve_json_data&rsquo; function is just a wrapper to call a URL and parse the returned JSON data:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">retrieve_json_data</span>(url):

    <span style="color:#66d9ef">try</span>:
        response <span style="color:#f92672">=</span> urllib2<span style="color:#f92672">.</span>urlopen(url)
    <span style="color:#66d9ef">except</span> urllib2<span style="color:#f92672">.</span>HTTPError, e:
        <span style="color:#66d9ef">raise</span> e
    <span style="color:#66d9ef">except</span> urllib2<span style="color:#f92672">.</span>URLError, e:
        <span style="color:#66d9ef">raise</span> e

    raw_data <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>read()
    data <span style="color:#f92672">=</span> json<span style="color:#f92672">.</span>loads(raw_data)

    <span style="color:#66d9ef">return</span> data</code></pre></div>

<p>Once I&rsquo;ve searched Spotify, I then want to see if the artist has a page on Last.FM. If they do, I also want to extract and store their top-tags from the site. Again, the Last.FM API makes this straightforward. Firstly, searching for the artist page:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">params <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#34;artist&#34;</span>: artist<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#34;utf-8&#34;</span>),
    <span style="color:#e6db74">&#34;api_key&#34;</span>: last_fm_api_key,
    <span style="color:#e6db74">&#34;method&#34;</span>: <span style="color:#e6db74">&#34;artist.getinfo&#34;</span>,
    <span style="color:#e6db74">&#34;format&#34;</span>: <span style="color:#e6db74">&#34;json&#34;</span>
}

last_fm_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://ws.audioscrobbler.com/2.0/?&#34;</span> <span style="color:#f92672">+</span> urllib<span style="color:#f92672">.</span>urlencode(params)

data <span style="color:#f92672">=</span> retrieve_json_data(last_fm_url)

<span style="color:#66d9ef">if</span> data<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;artist&#34;</span>, None) <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
    <span style="color:#66d9ef">if</span> data[<span style="color:#e6db74">&#34;artist&#34;</span>]<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;url&#34;</span>, None) <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        artist_data[<span style="color:#e6db74">&#34;last_fm_url&#34;</span>] <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#34;artist&#34;</span>][<span style="color:#e6db74">&#34;url&#34;</span>]</code></pre></div>

<p>Then, searching for the artist&rsquo;s top tags:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">params <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#34;artist&#34;</span>: artist<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#34;utf-8&#34;</span>),
    <span style="color:#e6db74">&#34;api_key&#34;</span>: last_fm_api_key,
    <span style="color:#e6db74">&#34;method&#34;</span>: <span style="color:#e6db74">&#34;artist.gettoptags&#34;</span>,
    <span style="color:#e6db74">&#34;format&#34;</span>: <span style="color:#e6db74">&#34;json&#34;</span>
}

last_fm_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://ws.audioscrobbler.com/2.0/?&#34;</span> <span style="color:#f92672">+</span> urllib<span style="color:#f92672">.</span>urlencode(params)

data <span style="color:#f92672">=</span> retrieve_json_data(last_fm_url)

<span style="color:#66d9ef">if</span> data<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;toptags&#34;</span>, None) <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:

    artist_data[<span style="color:#e6db74">&#34;tags&#34;</span>] <span style="color:#f92672">=</span> {}

    <span style="color:#66d9ef">if</span> data[<span style="color:#e6db74">&#34;toptags&#34;</span>]<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;tag&#34;</span>, None) <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        tags <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#34;toptags&#34;</span>][<span style="color:#e6db74">&#34;tag&#34;</span>]
        <span style="color:#66d9ef">if</span> type(tags) <span style="color:#f92672">==</span> type([]):
            <span style="color:#66d9ef">for</span> tag <span style="color:#f92672">in</span> tags:
                name <span style="color:#f92672">=</span> tag[<span style="color:#e6db74">&#34;name&#34;</span>]<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#39;utf-8&#39;</span>)
                count <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> int(tag[<span style="color:#e6db74">&#34;count&#34;</span>]) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> int(tag[<span style="color:#e6db74">&#34;count&#34;</span>])
                artist_data[<span style="color:#e6db74">&#34;tags&#34;</span>][name] <span style="color:#f92672">=</span> count
            <span style="color:#66d9ef">else</span>:
                name <span style="color:#f92672">=</span> tags[<span style="color:#e6db74">&#34;name&#34;</span>]<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#39;utf-8&#39;</span>)
                count <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> int(tags[<span style="color:#e6db74">&#34;count&#34;</span>]) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> int(tags[<span style="color:#e6db74">&#34;count&#34;</span>])
                artist_data[<span style="color:#e6db74">&#34;tags&#34;</span>][name] <span style="color:#f92672">=</span> count</code></pre></div>

<p>Again, once we&rsquo;ve retrieved all the extra artist data, we can dump it to file:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;bands.json&#34;</span>, <span style="color:#e6db74">&#34;w&#34;</span>) <span style="color:#66d9ef">as</span> outfile:
    json<span style="color:#f92672">.</span>dump(artists, outfile)</code></pre></div>

<p>So, I now have 2 scripts that I can run regularly to capture any updates to the festival website (including lineup additions) and to search for artist data on Spotify and Last.FM. Now I&rsquo;ve got all this data captured and stored, it&rsquo;s time to start doing something interesting with it&hellip;</p>

        </div>
    </article>
    
    <article class="blogpost">
        <header>
            <h1> <a href="/2013/08/14/swn-festival-2013-plans-part-1-the-data/">
                    SWN Festival 2013 plans -  part 1: the data
                </a></h1>
            <p class="author">
                <span class="date">Wednesday, Aug 14, 2013</span>
            </p>
        </header>
        <div class="content">
            <p>As <a href="/2013/08/11/swn-festival-2013-plans/">I mentioned</a>, I&rsquo;m planning on doing a bit more development work this year connected to the <a href="http://www.swnfest.com">SWN Festival</a>. The first stage is to get hold of the data associated with the festival in an accessible and machine readable form so it can be used in other apps.</p>

<p>Unfortunately (but unsurprisingly), being a smallish local festival, there is no API for any of the data. So, getting a list of the bands and their info means we need to resort to web scraping. Fortunately, with a couple of lines of python and the BeautifulSoup library, getting the list of artists playing the festival is pretty straightforward:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> urllib2
<span style="color:#f92672">import</span> json

<span style="color:#f92672">from</span> bs4 <span style="color:#f92672">import</span> BeautifulSoup
root_page <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://swnfest.com/&#34;</span>
lineup_page <span style="color:#f92672">=</span> root_page <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;lineup/&#34;</span>

<span style="color:#66d9ef">try</span>:
    response <span style="color:#f92672">=</span> urllib2<span style="color:#f92672">.</span>urlopen(lineup_page)
<span style="color:#66d9ef">except</span> urllib2<span style="color:#f92672">.</span>HTTPError, e:
    <span style="color:#66d9ef">raise</span> e
<span style="color:#66d9ef">except</span> urllib2<span style="color:#f92672">.</span>URLError, e:
    <span style="color:#66d9ef">raise</span> e

raw_data <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>read()

soup <span style="color:#f92672">=</span> BeautifulSoup(raw_data)

links <span style="color:#f92672">=</span> soup<span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#34;.artist-listing h5 a&#34;</span>)

artists <span style="color:#f92672">=</span> {}

<span style="color:#66d9ef">for</span> link <span style="color:#f92672">in</span> links:
    url <span style="color:#f92672">=</span> link<span style="color:#f92672">.</span>attrs[<span style="color:#e6db74">&#34;href&#34;</span>]
    artist <span style="color:#f92672">=</span> link<span style="color:#f92672">.</span>contents[<span style="color:#ae81ff">0</span>]

    artists[artist] <span style="color:#f92672">=</span> {}
    artists[artist][<span style="color:#e6db74">&#34;swn_url&#34;</span>] <span style="color:#f92672">=</span> url</code></pre></div>

<p>All we&rsquo;re doing here is loading the <a href="http://swnfest.com/lineup/">lineup</a> page for the main festival website, using BeautifulSoup to find all the links to individual artist pages (which are in a div with a class of &ldquo;artist-listing&rdquo;, each one in a h5 tag), then parsing these links to extract the artist name, and the url of their page on the festival website.</p>

<p>Each artist page on the website includes handy links to soundcloud, twitter, youtube etc (where these exist), and since I&rsquo;m going to want to include these kinds of things in the apps I&rsquo;m working on, I&rsquo;ll grab those too:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> artist, data <span style="color:#f92672">in</span> artists<span style="color:#f92672">.</span>iteritems():
    <span style="color:#66d9ef">try</span>:
        response <span style="color:#f92672">=</span> urllib2<span style="color:#f92672">.</span>urlopen(data[<span style="color:#e6db74">&#34;swn_url&#34;</span>])
    <span style="color:#66d9ef">except</span> urllib2<span style="color:#f92672">.</span>HTTPError, e:
        <span style="color:#66d9ef">raise</span> e
    <span style="color:#66d9ef">except</span> urllib2<span style="color:#f92672">.</span>URLError, e:
        <span style="color:#66d9ef">raise</span> e

    raw_data <span style="color:#f92672">=</span> response<span style="color:#f92672">.</span>read()

    soup <span style="color:#f92672">=</span> BeautifulSoup(raw_data)

    links <span style="color:#f92672">=</span> soup<span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#34;.outlinks li&#34;</span>)

    <span style="color:#66d9ef">for</span> link <span style="color:#f92672">in</span> links:
         source_name <span style="color:#f92672">=</span> link<span style="color:#f92672">.</span>attrs[<span style="color:#e6db74">&#34;class&#34;</span>][<span style="color:#ae81ff">0</span>]
         source_url <span style="color:#f92672">=</span> link<span style="color:#f92672">.</span>findChild(<span style="color:#e6db74">&#34;a&#34;</span>)<span style="color:#f92672">.</span>attrs[<span style="color:#e6db74">&#34;href&#34;</span>]
         data[source_name] <span style="color:#f92672">=</span> source_url</code></pre></div>

<p>This code iterates through the list of artists we just extracted from the lineup page, retrieves the relevant artist page, and parses it for the outgoing links, stored in list items in an unordered list with a class of &lsquo;outlinks&rsquo;. Fortunately each link in this list has a class describing what type of link it is (facebook/twitter/soundcloud etc) so we can use the class as a key in our dictionary, with the link itself as an item. Later on once schedule information is included in the artist page we can add some code to parse stage-times and venues, but at the moment that data isn&rsquo;t present on the pages, so we can&rsquo;t extract it yet.</p>

<p>Finally we can just dump our artist data to json, and we have the information we need in an easily accessible format:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;bands.json&#34;</span>, <span style="color:#e6db74">&#34;w&#34;</span>) <span style="color:#66d9ef">as</span> outfile:
    json<span style="color:#f92672">.</span>dump(artists, outfile)</code></pre></div>

<p>Now we have the basic data for each artist, we can go on to search for more information on other music sites. The nice thing about this script is that when the lineup gets updated, we can just re-run the code and capture all the new artists that have been added. I should also mention that all the code I&rsquo;m using for this is available on <a href="https://github.com/martinjc/swnScraper2013">github</a>.</p>

        </div>
    </article>
    
    

<ul class="pagination">
    
    <li class="page-item">
        <a href="/tags/python/" class="page-link" aria-label="First"><span aria-hidden="true">&laquo;&laquo;</span></a>
    </li>
    
    <li class="page-item disabled">
    <a href="" class="page-link" aria-label="Previous"><span aria-hidden="true">&laquo;</span></a>
    </li>
    
    
    
    
    
    
        
        
    
    
    <li class="page-item active"><a class="page-link" href="/tags/python/">1</a></li>
    
    
    
    
    
    
        
        
    
    
    <li class="page-item"><a class="page-link" href="/tags/python/page/2/">2</a></li>
    
    
    <li class="page-item">
    <a href="/tags/python/page/2/" class="page-link" aria-label="Next"><span aria-hidden="true">&raquo;</span></a>
    </li>
    
    <li class="page-item">
        <a href="/tags/python/page/2/" class="page-link" aria-label="Last"><span aria-hidden="true">&raquo;&raquo;</span></a>
    </li>
    
</ul>

</main>

    </div><script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-25500481-1', 'auto');
    ga('send', 'pageview');

</script>
<script>window.twttr = (function (d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0],
            t = window.twttr || {};
        if (d.getElementById(id)) return t;
        js = d.createElement(s);
        js.id = id;
        js.src = "https://platform.twitter.com/widgets.js";
        fjs.parentNode.insertBefore(js, fjs);

        t._e = [];
        t.ready = function (f) {
            t._e.push(f);
        };

        return t;
    }(document, "script", "twitter-wjs"));
</script></body>

</html>